Project Discussion: AI-Driven Decision AssistantThis document details the design decisions and methodology employed during the solution of this project, ensuring all "Anti-Triviality" requirements are met.1. ML Methodology and Data IntegrityDecision on Target Column BehaviorDuring synthetic data generation, a question arose regarding the persistence of the target column (lapse_next_3m) after a policy has officially lapsed. A policy that lapses (value 1) may transition to an 'inactive' status, sometimes resulting in a sequence like ...0, 0, 1, 1, 1, 0, 0... in the temporal panel data, where the zeros following the '1's represent the policy's final months of inactivity or potential reinstatement attempts.Decision: The data points representing policy months after the confirmed lapse event were intentionally retained in the dataset.Rationale: In a real-world scenario, data is seldom clean. These records may represent edge cases (e.g., policy reinstatement, delayed administrative cleanup). Excluding them would oversimplify the modeling task. Their inclusion challenges the model to learn the onset of the lapse event, rather than merely identifying policies that have already entered a post-lapse state, thereby increasing the model's robustness.Model Performance (ML Soundness)After running the models (LGBM and XGBoost) with light tuning, the performance metrics (as measured by AUC-PR and Precision@K) are acknowledged to be moderate.Conscious Decision: I am aware of these results and made a conscious choice not to pursue further optimization or advanced feature engineering (e.g., complex lag features, feature crosses).Rationale: The primary objective was to deliver a complete, robust, end-to-end (E2E) solution pipeline that adheres to the strict build time target (â‰¤2 hours) and runtime discipline (<5 minutes). While feature engineering would undoubtedly improve the metric scores, the focus was placed on building a stable, reproducible ML/RAG infrastructure, not on achieving State-of-the-Art (SOTA) performance.SHAP ResultsThe global SHAP plot (saved at out/shap_plot.png) illustrates global feature importance. As expected in churn/lapse modeling, features like policy premium, policy tenure_m (duration), and customer age were identified as the most influential factors driving the lapse prediction.2. Anti-Triviality RequirementsData Leakage Safeguard: A specific "trap" feature, post_event_retention_effort, was identified. This feature represents an action taken after the lapse event (e.g., a retention call). Including it would cause severe target leakage. Consequently, this feature was explicitly excluded from the final feature set (FEATURE_COLS) before model training.Temporal Integrity: A strict temporal split (Train/Validation/Test) was executed based on fixed date cutoffs (TRAIN_END_DATE, VALIDATION_END_DATE). The logic in data_prep.py validates that there is no time overlap between these three sets. This integrity was programmatically verified by inspecting the minimum and maximum dates of each subset post-split.RAG Faithfulness: Every generated RAG output (for both lapse prevention and leads) is grounded and includes source citations (e.g., [Doc 1], [Doc 2]). This is achieved by explicitly instructing the LLM (or the fallback function) to base its answer only on the provided context and to cite the sources from which the information was retrieved.Probability-in-Prompt: In the lapse prevention branch, the predicted lapse probability (lapse_proba) for each customer is injected directly into the RAG prompt. For example: "The customer... has a High Lapse Risk (85.2%)." This ensures the RAG strategy is specifically tailored to the calculated risk level.Determinism and Reproducibility: The entire pipeline (data generation, splitting, model tuning) utilizes a consistent SEED = 42. This guarantees that every repeated execution of python run.py yields identical results, including the same data, model parameters, and final metrics.Runtime Discipline: The complete end-to-end pipeline executes in under 5 minutes (typically 1-2 minutes in local testing), successfully meeting the time constraint.